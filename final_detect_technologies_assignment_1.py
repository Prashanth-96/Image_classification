# -*- coding: utf-8 -*-
"""Final Detect_Technologies_Assignment_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZK0uxUAvQWuQ6p_taKlBLRU1QB3QVrB0
"""

#Importing Necessary Packages
import matplotlib.pyplot as plt
import seaborn as sns
import keras
from keras.models import Sequential
from keras.layers import Activation, BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPooling2D
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam
from sklearn.metrics import classification_report,confusion_matrix
from keras.regularizers import l2
from keras.losses import CategoricalCrossentropy
import tensorflow as tf
import cv2
import os
import pandas as pd
import numpy as np
from collections import Counter
from keras.utils import to_categorical
import shutil
from loguru import logger
from keras.callbacks import ReduceLROnPlateau, EarlyStopping

!pip install loguru

from google.colab import drive
drive.mount('/content/drive')

train_dataset_path = '/content/drive/MyDrive/nature_12K/inaturalist_12K/train'
validation_dataset_path = '/content/drive/MyDrive/nature_12K/inaturalist_12K/val'

class_name_counts_training={}
for i in os.listdir(train_dataset_path):
  if os.path.isdir(train_dataset_path+'/'+i):
    class_name_counts_training[i]=len(os.listdir(train_dataset_path+'/'+i))
class_name_counts_training

class_name_counts_val={}
for i in os.listdir(validation_dataset_path):
  if os.path.isdir(validation_dataset_path+'/'+i):
    class_name_counts_val[i]=len(os.listdir(validation_dataset_path+'/'+i))
class_name_counts_val

"""As the Dataset size is High we are now doing a random sampling technique to sample the image for training"""

training_sampled_images={}
for folders in os.listdir(train_dataset_path):
  try:
    if os.path.isdir(train_dataset_path+'/'+folders):
      count=0
      image_lst=[]
      img_folder=train_dataset_path+'/'+folders
      for img in os.listdir(img_folder):
        count=count+1
        if count<=300:
          image_lst.append(img_folder+'/'+img)
      logger.info('Done for '+folders)
      training_sampled_images[folders]=image_lst
  except Exception as e:
    print('Failed Due to '+e)

for key,val in training_sampled_images.items():
  if os.path.exists('/content/drive/MyDrive/train_DT2')==False:
    os.mkdir('/content/drive/MyDrive/train_DT2')
  if os.path.exists('/content/drive/MyDrive/train_DT2/'+key)==False:
    os.mkdir('/content/drive/MyDrive/train_DT2/'+key)
  for i in val:
    shutil.copy(i,'/content/drive/MyDrive/train_DT2/'+key)

#Checking the Number of images
for i in os.listdir('/content/drive/MyDrive/train_DT2'):
   imgs = len(os.listdir('/content/drive/MyDrive/train_DT2/'+i))
   print('The number of Images in '+i+' is '+str(imgs))

"""CATEGORIES AVAILABLE UNDER TRAINING and VALIDATIONS DATA"""

New_training_path='/content/drive/MyDrive/train_DT2'
validation_path='/content/drive/MyDrive/nature_12K/inaturalist_12K/val'

x_train=[]
for folder in os.listdir(New_training_path):
  if os.path.isdir(New_training_path+"/"+folder):
    sub_path=New_training_path+"/"+folder
    logger.info("Entering "+folder)
    for img in os.listdir(sub_path):
      try:
        image_path=sub_path+"/"+img
        img_arr=cv2.imread(image_path)
        img_arr=cv2.resize(img_arr,(224,224))
        x_train.append(img_arr)
      except Exception as e:
        print('Failed at '+img+' Due to '+str(e))

#For testing
x_val=[]
for folder in os.listdir(validation_path):
    sub_path=validation_path+"/"+folder
    try:
      for img in os.listdir(sub_path):
          image_path=sub_path+"/"+img
          img_arr=cv2.imread(image_path)
          img_arr=cv2.resize(img_arr,(224,224))
          x_val.append(img_arr)
    except Exception as e:
      print('Failed at '+img+' because of '+str(e))

train_x=np.array(x_train)
val_x=np.array(x_val)
train_x=train_x/255.0
val_x=val_x/255.0

train_datagen = ImageDataGenerator(rescale = 1./255)
val_datagen = ImageDataGenerator(rescale = 1./255)
training_set = train_datagen.flow_from_directory(New_training_path,
                                                 target_size = (224, 224),
                                                 batch_size = 32,
                                                 class_mode = 'categorical')
val_set = val_datagen.flow_from_directory(validation_path,
                                            target_size = (224, 224),
                                            batch_size = 32,
                                            class_mode = 'categorical')

train_y=training_set.classes
val_y=val_set.classes

training_set.class_indices

train_y.shape

val_y.shape

model = Sequential()
model.add(Conv2D(32,3,padding="same", activation="relu", input_shape=(224,224,3)))
model.add(MaxPooling2D())
model.add(Dropout(0.4))
model.add(Flatten())
model.add(Dense(10, activation="softmax"))
model.summary()

# model = Sequential()
# model.add(Conv2D(32,3,padding="same", activation="relu", input_shape=(224,224,3)))
# model.add(MaxPooling2D())
# model.add(Conv2D(16, 3, padding="same", activation="relu"))
# model.add(MaxPooling2D())
# model.add(Dropout(0.4))
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))
# model.summary()

optimizer = Adam(learning_rate=0.1)

model.compile(optimizer=optimizer, loss=CategoricalCrossentropy(), metrics=['accuracy'])

val_x.shape

y_train2 = to_categorical(train_y, 10)

val_y2=to_categorical(val_y, 10)

history = model.fit(train_x,y_train2, epochs=5,validation_data=(val_x,val_y2),
                       verbose=2)